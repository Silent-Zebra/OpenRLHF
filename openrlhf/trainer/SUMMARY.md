# OpenRLHF Trainer Summary

This document summarizes the trainer classes found in the `openrlhf/trainer` directory. These trainers encapsulate the logic for various reinforcement learning and fine-tuning algorithms used in OpenRLHF.

## `CombinedHarmlessnessTrainer` (`combined_harmlessness_trainer.py`)

### Purpose

This trainer implements an advanced adversarial alignment algorithm focused on improving model harmlessness. It orchestrates a two-part training process involving a "base actor" and a "sampling actor" (also referred to as a proposal or twist model). The goal is to train the base actor to be harmless while using the sampling actor to explore and generate potentially harmful content, which is then used to guide the base actor's training, often through specialized loss functions like REINFORCE or negative training variations. This approach differs from simpler PPO by explicitly modeling and sampling from an approximate target distribution related to harmlessness.

### Core Functionality

1.  **Dual Model Setup:** Manages two distinct actor models:
    *   `base_actor`: The primary model being trained for harmlessness.
    *   `sampling_actor`: A model trained to generate challenging prompts or completions (potentially harmful, approximating a target distribution `p* \propto p_0 e^{-beta r}`) to train the `base_actor` against. It might use different parameterizations (e.g., modulation) relative to the `base_actor`.
2.  **Experience Generation:** Uses separate `ExperienceMaker` instances for both actors.
    *   `base_experience_maker`: Generates experiences (prompts, actions, rewards, logprobs) using the `base_actor`. Rewards are typically calculated using a standard reward model.
    *   `sampling_experience_maker_neg`: Generates experiences using the `sampling_actor`, potentially focusing on generating "negative" or challenging examples. It might use importance sampling weights based on the reward model and the `target_dist_beta` parameter.
3.  **Alternating Training Loop (`fit` method):**
    *   Iterates through epochs and updates.
    *   Generates rollouts/experiences using both the `base_actor` and `sampling_actor`.
    *   Performs training steps for both actors and their corresponding critics (if used).
4.  **Actor Training (`training_step_base_actor`, `training_step_sampling_actor`):**
    *   Calculates losses for each actor based on configured loss types (`base_actor_loss_type`, `sampling_actor_loss_type`).
    *   Supported loss types for the `base_actor` include `reinforce`, `neg_training`, and `neg_reinforce`, which incorporate rewards and potentially negative examples.
    *   Supported loss types for the `sampling_actor` include `ctl` (Contrastive Tension Learning) and `dpg` (Derived Policy Gradient), designed to train the proposal distribution.
    *   Performs backpropagation and optimizer steps.
5.  **Critic Training (`training_step_critic`, if critics are used):** Calculates and optimizes value loss (e.g., MSE) for the critics associated with each actor.
6.  **Loss Calculation:** Implements specific loss functions (`get_base_actor_loss`, `get_sampling_actor_loss`) tailored to the dual-actor setup and harmlessness objectives. This often involves complex weighting or combination of rewards and log probabilities from both actors.
7.  **Parameterization Handling:** Supports different ways the `sampling_actor`'s parameters relate to the `base_actor` (e.g., full model, modulation).
8.  **Checkpointing:** Saves separate checkpoints for the base and proposal/sampling models.

### Key Arguments & Concepts

*   `base_actor`, `sampling_actor`: The two core models.
*   `base_critic`, `sampling_critic`: Optional value function models.
*   `reward_model`, `static_initial_model`, `ema_model`: Standard components for reward calculation, KL reference, and stability.
*   `base_actor_optim`, `sampling_actor_optim`, etc.: Optimizers and schedulers for each component.
*   `base_actor_loss_type`: Loss for the main harmless model (e.g., 'reinforce', 'neg_training').
*   `sampling_actor_loss_type`: Loss for the proposal model (e.g., 'ctl', 'dpg').
*   `target_dist_beta`: Controls the "temperature" of the target distribution approximated by the sampling actor.
*   `parameterization`: Defines how the sampling actor relates to the base actor.
*   `alpha`: Weighting parameter used in some negative training losses.

## `HarmlessnessTrainer` (`harmlessness_trainer.py`)

### Purpose

This trainer implements harmlessness training algorithms using techniques like REINFORCE, Negative Training, or a combination (`NegREINFORCE`). Unlike the `CombinedHarmlessnessTrainer`, it appears to focus on training a single primary `base_actor` using experiences potentially generated by both the `base_actor` itself and a separate `sampling_actor` (which might explore less desirable behaviors).

### Core Functionality

1.  **Model Setup:** Manages a `base_actor` (the model being trained), an optional `sampling_actor` (for generating supplementary/negative experiences), a `critic` (optional, for value estimation), a `reward_model`, an `initial_model` (for reference/KL), and an `ema_model`.
2.  **Experience Generation:** Uses two `BaseExperienceMaker` instances:
    *   `experience_maker`: Generates experiences using the `base_actor` and standard reward signals (`target_dist_beta=1`).
    *   `experience_maker_neg_sampling`: Generates experiences using the `sampling_actor`. This maker uses the `target_dist_beta` parameter, suggesting it generates samples approximating a specific target distribution, likely related to negative or harmful examples, potentially using importance sampling based on rewards.
3.  **Training Loop (`fit` method):**
    *   Iterates through epochs and updates.
    *   Generates experiences using both experience makers.
    *   Performs training steps for the actor and critic (if used).
4.  **Actor Training (`training_step_actor`):**
    *   Calculates the actor loss using the specified `actor_loss_type` (`reinforce`, `neg_training`, `neg_reinforce`).
    *   The loss function (`get_actor_loss`) combines information from both the standard experiences (`experience`) and the negative sampling experiences (`experience_neg`). The specific combination depends on the chosen loss type (e.g., `NegREINFORCE` uses rewards/logprobs from both).
    *   Performs backpropagation and optimizer steps for the `base_actor`.
5.  **Critic Training (`training_step_critic`, if critic exists):** Calculates and optimizes the value loss (e.g., MSE) for the critic based on the standard experiences.
6.  **Loss Calculation:** Implements loss functions (`get_actor_loss`, `get_critic_loss`) that potentially integrate standard RL signals with signals derived from the negative sampling process.
7.  **Checkpointing:** Saves model checkpoints.

### Key Arguments & Concepts

*   `base_actor`: The primary model being trained.
*   `sampling_actor`: An auxiliary model used to generate targeted (potentially negative) samples.
*   `critic`: Optional value function model.
*   `actor_loss_type`: Specifies the core training algorithm ('reinforce', 'neg_training', 'neg_reinforce').
*   `target_dist_beta`: Controls the distribution targeted by the `sampling_actor`'s experience maker.
*   `alpha`: Weighting parameter for negative training losses.
*   `baseline_type`, `hardcoded_baseline`: Options for reward baselines in REINFORCE-style losses.

## `DPOTrainer` (`dpo_trainer.py`)

### Purpose

This trainer implements the Direct Preference Optimization (DPO) algorithm. DPO is a method for aligning language models with human preferences without explicitly training a separate reward model. It directly optimizes the language model to increase the likelihood of preferred responses and decrease the likelihood of dispreferred responses, using a reference model for regularization.

### Core Functionality

1.  **Model Setup:** Manages the primary `model` being trained and a frozen `ref_model` (reference model). It also handles the optimizer, scheduler, and data loaders.
2.  **Data Handling:** Processes preference data, which typically consists of a prompt, a chosen (preferred) response, and a rejected (dispreferred) response. Supports standard datasets and packed datasets (`packing_samples=True`) for efficiency.
3.  **Forward Pass (`concatenated_forward` / `packed_samples_forward`):**
    *   Computes log probabilities for both the chosen and rejected responses using both the main `model` and the `ref_model`.
    *   Handles concatenation or packing/unpacking of chosen and rejected sequences for efficient batch processing.
    *   Can optionally compute auxiliary loss (e.g., for Mixtral MoE models) and NLL loss on the chosen response.
4.  **Loss Calculation (`loss_fn` - `DPOLoss`):**
    *   Calculates the DPO loss, which involves the difference between the log probability ratios (model vs. reference) for the chosen and rejected responses. The loss encourages the model's log probability for the chosen response (relative to the reference) to be higher than that for the rejected response.
    *   The `beta` parameter controls the strength of the KL divergence regularization against the reference model.
    *   Supports label smoothing and IPO (Identity Preference Optimization) variants.
5.  **Training Loop (`fit` method):**
    *   Iterates through epochs and batches of preference data.
    *   Performs the forward pass to get log probabilities.
    *   Calculates the combined loss (DPO loss + optional auxiliary loss + optional NLL loss).
    *   Performs backpropagation and optimizer/scheduler steps using the `strategy` (e.g., DeepSpeed).
    *   Logs metrics like loss, accuracy (fraction of pairs where chosen has higher reward proxy), chosen/rejected reward proxies, and learning rate.
6.  **Evaluation (`evaluate` method):**
    *   Calculates the DPO loss and accuracy on the evaluation dataset.
    *   Logs evaluation metrics.
7.  **Log Probability Calculation (`_get_batch_logps`, `_packed_get_batch_logps`):** Helper functions to compute sequence log probabilities from model logits, correctly handling padding and prompt masking.
8.  **Checkpointing & Logging:** Integrates with `wandb` or `tensorboard` for logging and saves checkpoints using the `strategy`.

### Key Arguments & Concepts

*   `model`: The policy model being trained.
*   `ref_model`: A frozen copy of the initial model used for regularization.
*   `beta`: Coefficient controlling the KL divergence penalty against the `ref_model`.
*   `label_smoothing`: Applies label smoothing to the preference labels (0 for rejected, 1 for chosen).
*   `ipo`: Flag to use the Identity Preference Optimization loss variant.
*   `aux_loss_coef`: Coefficient for auxiliary loss (e.g., Mixtral MoE balancing loss).
*   `nll_loss_coef`: Coefficient for adding a negative log-likelihood loss term on the chosen responses.
*   `packing_samples`: Enables packing multiple sequences into one for potentially faster training.

## `KDTrainer` (`kd_trainer.py`)

### Purpose

This trainer implements Knowledge Distillation (KD) for language models. The goal is to train a smaller or target model (the "student") to mimic the behavior of a larger, more capable model (the "teacher"). This is achieved by minimizing a loss function that includes both a standard language modeling loss (e.g., cross-entropy on ground truth labels) and a distillation loss that encourages the student's output distribution (logits) to match the teacher's output distribution.

### Core Functionality

1.  **Model Setup:** Manages the student `model` being trained and a frozen `teacher_model`. It also handles the optimizer, scheduler, and data loaders.
2.  **Data Handling:** Processes standard language modeling datasets (sequences of tokens).
3.  **Forward Pass:**
    *   Computes the logits from the student `model` for the input sequences.
    *   Computes the standard next-token prediction loss (`GPTLMLoss`) using the student's logits and the ground truth labels (masking out prompt tokens if not in pretrain mode).
    *   Computes the logits from the frozen `teacher_model` for the same input sequences.
    *   Calculates the distillation loss (`KDLoss`) which typically measures the divergence (e.g., KL divergence) between the student's and teacher's predicted logit distributions over the vocabulary, applied only to the response tokens.
4.  **Combined Loss:** Calculates a final loss by combining the standard GPT loss and the distillation loss, weighted by `(1 - kd_coef)` and `kd_coef` respectively.
5.  **Training Loop (`fit` method):**
    *   Iterates through epochs and batches of data.
    *   Performs the forward pass to get student and teacher logits.
    *   Calculates the combined loss.
    *   Performs backpropagation and optimizer/scheduler steps for the student `model` using the `strategy`.
    *   Logs metrics like the GPT loss component, the distillation loss component, and the learning rate.
6.  **Evaluation (`evaluate` method):**
    *   Calculates the standard GPT loss (next-token prediction loss) on the evaluation dataset using the student model.
    *   Logs the evaluation loss.
7.  **Checkpointing & Logging:** Integrates with `wandb` or `tensorboard` for logging and saves checkpoints using the `strategy`.

### Key Arguments & Concepts

*   `model`: The student model being trained.
*   `teacher_model`: The frozen, larger/better model providing distillation targets.
*   `kd_coef`: The coefficient weighting the distillation loss term in the combined loss function.
*   `GPTLMLoss`: Standard cross-entropy loss for next-token prediction.
*   `KDLoss`: Loss function measuring the divergence between student and teacher output distributions.

## `KTOTrainer` (`kto_trainer.py`)

### Purpose

This trainer implements Kahneman-Tversky Optimization (KTO), an alignment algorithm that uses a different approach than DPO or reward modeling. KTO aligns language models based on whether an output for a given prompt is considered "desirable" (good) or "undesirable" (bad), rather than requiring pairwise preferences. It optimizes the model to increase the likelihood of desirable outputs and decrease the likelihood of undesirable outputs, relative to a reference model.

### Core Functionality

1.  **Model Setup:** Manages the primary `model` being trained and a frozen `ref_model` (reference model), similar to DPO. Handles optimizer, scheduler, and data loaders.
2.  **Data Handling:** Processes datasets where each example consists of a prompt, a response, and a binary label indicating whether the response is desirable (`label=True` or 1) or undesirable (`label=False` or 0).
3.  **Forward Pass & Log Probability Calculation (`compute_model_logps`, `compute_model_logps_with_KL`):**
    *   Computes the log probabilities of the provided response tokens given the prompt, using both the main `model` and the `ref_model`.
    *   Calculates the KL divergence between the model's output distribution and the reference model's distribution for each example.
    *   Can optionally compute auxiliary loss (e.g., for Mixtral).
4.  **Loss Calculation (`loss_fn` - `KTOLoss`):**
    *   Calculates the KTO loss based on the model's and reference model's log probabilities for the responses, the KL divergence, and the desirability labels.
    *   The loss function encourages higher log probabilities (relative to the reference model, controlled by `beta`) for desirable examples and lower log probabilities for undesirable examples.
    *   Uses separate weighting factors (`desirable_loss_weight`, `undesirable_loss_weight`) for the two types of examples.
5.  **Training Loop (`fit` method):**
    *   Iterates through epochs and batches of KTO data (prompt, response, label).
    *   Performs the forward pass to get log probabilities and KL divergence for both models.
    *   Calculates the combined loss (KTO loss + optional auxiliary loss).
    *   Performs backpropagation and optimizer/scheduler steps using the `strategy`.
    *   Logs metrics like KTO loss, KL divergence, average implicit rewards for chosen/rejected samples, and learning rate.
6.  **Evaluation (`evaluate` method):**
    *   Calculates the KTO loss and related metrics on the evaluation dataset.
    *   Logs evaluation metrics.
7.  **Checkpointing & Logging:** Integrates with `wandb` or `tensorboard` and saves checkpoints.

### Key Arguments & Concepts

*   `model`: The policy model being trained.
*   `ref_model`: A frozen copy of the initial model used for regularization.
*   `beta`: Coefficient controlling the KL divergence penalty against the `ref_model` within the KTO loss calculation.
*   `desirable_loss_weight`: Weight applied to the loss term for desirable examples.
*   `undesirable_loss_weight`: Weight applied to the loss term for undesirable examples.
*   `KTOLoss`: The specific loss function implementing the KTO objective.

## `BasePPOTrainer` (`ppo_trainer.py`)

### Purpose

This trainer implements the Proximal Policy Optimization (PPO) algorithm, a popular reinforcement learning technique often used for RLHF (Reinforcement Learning from Human Feedback). It trains an `actor` (policy) model to generate responses that maximize rewards obtained from a `reward_model`, while staying close to an `initial_model` (reference) to prevent policy collapse, regulated by a KL divergence penalty. It also trains a `critic` model to estimate the value function (expected future rewards), which helps reduce the variance of policy gradient updates.

This base trainer also includes functionality for more advanced RL techniques beyond standard PPO, such as CTL (Contrastive Tension Learning), SIXO (Simplified Infinite Horizon Optimization), and DPG (Derived Policy Gradient), which can be selected via the `actor_loss_type` and `critic_loss_type` arguments.

### Core Functionality

1.  **Model Setup:** Manages the `actor`, `critic`, `reward_model`, `initial_model` (for KL penalty and reference logprobs), and an optional `ema_model` (Exponential Moving Average of the actor for potentially more stable generation/evaluation).
2.  **Experience Generation (`experience_maker` - `BaseExperienceMaker`):**
    *   Generates trajectories (sequences) by rolling out the current `actor` policy starting from prompts.
    *   Calculates rewards for the generated sequences using the `reward_model` (potentially remote or a custom function).
    *   Computes log probabilities of the actions taken by the `actor` and the `initial_model`.
    *   Calculates KL divergence between the actor and initial model.
    *   Computes advantages (using Generalized Advantage Estimation - GAE) and returns based on rewards and `critic` value estimates.
    *   Stores these experiences (sequences, logprobs, values, rewards, advantages) in a `NaiveReplayBuffer`.
3.  **Training Loop (`fit` method):**
    *   Iterates through episodes/updates.
    *   Collects rollouts using the `experience_maker` and fills the `replay_buffer`.
    *   Enters a PPO optimization phase (`ppo_train`) where it iterates multiple epochs over the collected buffer data.
4.  **PPO Optimization (`ppo_train` method):**
    *   For each epoch, iterates through the `replay_buffer` in mini-batches.
    *   Performs training steps for the actor (`training_step_actor`) and critic (`training_step_critic`).
5.  **Actor Training (`training_step_actor`):**
    *   Calculates the actor loss based on the chosen `actor_loss_type` (PPO, CTL, SIXO, DPG).
        *   **PPO Loss (`PolicyLoss`):** Uses clipped surrogate objective based on probability ratios and advantages.
        *   **CTL/SIXO/DPG Losses:** Implement alternative RL objectives often involving contrastive learning or different ways of leveraging the value function and reference model.
    *   Optionally adds a pre-training loss (`ptx_loss_fn` - `GPTLMLoss`) on supervised data.
    *   Performs backpropagation and optimizer/scheduler steps for the actor.
    *   Updates the KL controller (`kl_ctl`) based on the measured KL divergence (for adaptive KL penalty).
6.  **Critic Training (`training_step_critic`):**
    *   Calculates the critic loss based on the chosen `critic_loss_type` (MSE, CTL, SIXO, Mixed).
        *   **MSE Loss (`ValueLoss`):** Standard mean squared error between predicted values and calculated returns, potentially with value clipping.
        *   **CTL/SIXO/Mixed Losses:** Implement alternative value function objectives corresponding to the actor losses.
    *   Performs backpropagation and optimizer/scheduler steps for the critic.
7.  **KL Control:** Uses `AdaptiveKLController` or `FixedKLController` to manage the KL penalty coefficient, either adapting it towards a target KL or keeping it fixed.
8.  **Parameterization Handling:** Supports different parameterizations via `get_log_psi_policy_parameterization`, potentially for techniques like twist/proposal models (though primarily used in `CombinedHarmlessnessTrainer`).
9.  **Checkpointing & Logging:** Integrates with `wandb` or `tensorboard` and saves checkpoints.

### Key Arguments & Concepts

*   `actor`, `critic`, `reward_model`, `initial_model`, `ema_model`: Core model components.
*   `actor_loss_type`: Selects the RL algorithm ('ppo', 'ctl', 'sixo', 'dpg').
*   `critic_loss_type`: Selects the value function loss ('mse', 'ctl', 'sixo', 'mixed_ctl_mse').
*   `kl_coef`, `kl_target`: Controls the KL divergence penalty between the actor and initial model.
*   `eps_clip`, `value_clip`: PPO-specific clipping parameters.
*   `vf_coef`: Weighting coefficient for the value loss in the total loss.
*   `ptx_coef`: Weighting coefficient for the pre-training loss.
*   `ExperienceMaker`, `NaiveReplayBuffer`: Utilities for generating and storing RL experience.
*   `PolicyLoss`, `ValueLoss`, `CTLLoss`, `SIXOLoss`, `DPGLoss`: Specific loss function implementations.

## `ProcessRewardModelTrainer` (`prm_trainer.py`)

### Purpose

This trainer is designed for training a Process Reward Model (PRM). Unlike traditional outcome-based reward models that assign a single score to a complete response, a PRM evaluates the generation process step-by-step. It predicts a reward score for each token generated by the language model, aiming to capture finer-grained aspects of quality or preference throughout the response.

### Core Functionality

1.  **Model Setup:** Manages the PRM `model` being trained, optimizer, scheduler, and data loaders.
2.  **Data Handling:** Processes datasets specifically formatted for PRM training. Each example typically includes the input sequence (prompt + response) and labels indicating the reward signal at specific token positions (often associated with special placeholder tokens or specific reward tokens).
3.  **Forward Pass:**
    *   Computes logits from the PRM `model` for the input sequence.
    *   Can handle standard or packed sequences.
    *   Optionally computes auxiliary loss (e.g., for Mixtral).
4.  **Loss Calculation (`loss_fn` - `PRMLoss`):**
    *   Calculates the PRM loss, which typically involves predicting reward values only at specific token positions indicated by the labels (e.g., at placeholder tokens or designated reward tokens). The loss (often cross-entropy or MSE depending on the specific PRM formulation) measures the difference between the model's predicted reward logits/values and the target labels at these positions.
    *   Calculates accuracy based on the predicted reward signals.
5.  **Training Loop (`fit` method):**
    *   Iterates through epochs and batches of PRM data.
    *   Performs the forward pass.
    *   Calculates the combined loss (PRM loss + optional auxiliary loss).
    *   Performs backpropagation and optimizer/scheduler steps using the `strategy`.
    *   Logs metrics like PRM loss, accuracy, and learning rate.
6.  **Evaluation (`evaluate` method):**
    *   Calculates the PRM loss and accuracy on the evaluation dataset.
    *   Logs evaluation metrics.
7.  **Checkpointing & Logging:** Integrates with `wandb` and saves checkpoints.

### Key Arguments & Concepts

*   `model`: The Process Reward Model being trained.
*   `PRMLoss`: The loss function specifically designed for PRM, focusing on predictions at designated token positions.
*   `placeholder_token_id`, `reward_token_ids`: Special token IDs used in the data to indicate where reward predictions should be made and evaluated.
*   `packing_samples`: Enables packing multiple sequences into one for potentially faster training.

## `RewardModelTrainer` (`rm_trainer.py`)

### Purpose

This trainer is used to train a standard Reward Model (RM). An RM is typically a language model fine-tuned to predict a scalar reward score given an input (prompt + response). This score reflects human preferences, with higher scores assigned to better or preferred responses. The trained RM is crucial for RLHF methods like PPO, providing the reward signal to guide the policy model's training.

### Core Functionality

1.  **Model Setup:** Manages the RM `model` being trained, optimizer, scheduler, and data loaders.
2.  **Data Handling:** Processes pairwise preference datasets. Each example consists of a prompt, a chosen (preferred) response, and a rejected (dispreferred) response. It can also handle an optional margin value for margin-based losses.
3.  **Forward Pass (`concatenated_forward` / `packed_samples_forward`):**
    *   Computes scalar reward scores for both the chosen and rejected responses using the RM `model`.
    *   Handles concatenation or packing/unpacking of chosen and rejected sequences for efficient batch processing.
    *   Can optionally compute auxiliary loss (e.g., for Mixtral).
4.  **Loss Calculation (`loss_fn` - `PairWiseLoss` or `LogExpLoss`):**
    *   Calculates a pairwise loss that encourages the reward score of the chosen response (`chosen_reward`) to be higher than the score of the rejected response (`reject_reward`).
    *   `PairWiseLoss`: Often uses a logistic sigmoid function applied to the reward difference (`chosen_reward - reject_reward`), aiming to maximize the probability of the chosen response being preferred.
    *   `LogExpLoss`: Another common pairwise loss formulation.
    *   Can optionally incorporate a margin (`margin`) into the loss, requiring the chosen reward to exceed the rejected reward by at least the margin.
5.  **Training Loop (`fit` method):**
    *   Iterates through epochs and batches of preference data.
    *   Performs the forward pass to get reward scores.
    *   Calculates the combined loss (preference loss + optional auxiliary loss).
    *   Performs backpropagation and optimizer/scheduler steps using the `strategy`.
    *   Logs metrics like preference loss, accuracy (fraction of pairs where chosen reward > rejected reward), average chosen/rejected rewards, and learning rate.
6.  **Evaluation (`evaluate` method):**
    *   Calculates the preference loss and accuracy on the evaluation dataset.
    *   Logs evaluation metrics.
7.  **Checkpointing & Logging:** Integrates with `wandb` or `tensorboard` and saves checkpoints.

### Key Arguments & Concepts

*   `model`: The Reward Model being trained.
*   `loss`: Specifies the pairwise loss function ('sigmoid' for `PairWiseLoss`, or `LogExpLoss`).
*   `PairWiseLoss`, `LogExpLoss`: Implementations of common pairwise preference loss functions.
*   `margin_loss`: Boolean flag to enable margin-based loss calculation.
*   `packing_samples`: Enables packing multiple sequences into one for potentially faster training.

## `SFTTrainer` (`sft_trainer.py`)

### Purpose

This trainer implements Supervised Fine-Tuning (SFT) for language models. SFT is a standard technique used to adapt a pre-trained language model to specific downstream tasks or datasets by training it on examples of desired input-output behavior. It uses a standard language modeling objective (next-token prediction) but typically masks the loss computation on the input/prompt part of the sequence, focusing the training on generating the desired response.

### Core Functionality

1.  **Model Setup:** Manages the `model` being fine-tuned, optimizer, scheduler, and data loaders.
2.  **Data Handling:** Processes datasets consisting of input sequences (prompt + desired response). Handles both standard and packed sequence formats.
3.  **Forward Pass:**
    *   Computes logits from the `model` for the input sequences.
    *   Supports standard attention and Ring Attention (`ring_attn_group`).
    *   Optionally computes auxiliary loss (e.g., for Mixtral).
4.  **Loss Calculation (`loss_fn` - `GPTLMLoss`):**
    *   Calculates the standard cross-entropy loss for next-token prediction.
    *   **Crucially**, masks the loss calculation on the prompt tokens. If not in `pretrain_mode`, it sets the labels corresponding to the prompt tokens (identified by `prompt_id_lens` or `response_ranges` for packed/multiturn data) to an ignore index (`-100`), so only the response tokens contribute to the loss and gradients.
5.  **Training Loop (`fit` method):**
    *   Iterates through epochs and batches of SFT data.
    *   Performs the forward pass.
    *   Calculates the masked language modeling loss (+ optional auxiliary loss).
    *   Performs backpropagation and optimizer/scheduler steps using the `strategy`.
    *   Logs metrics like the GPT loss and learning rate.
6.  **Evaluation (`evaluate` method):**
    *   Calculates the masked language modeling loss on the evaluation dataset.
    *   Logs the evaluation loss.
7.  **Checkpointing & Logging:** Integrates with `wandb` or `tensorboard` and saves checkpoints (optionally in both DeepSpeed and Hugging Face formats).

### Key Arguments & Concepts

*   `model`: The language model being fine-tuned.
*   `GPTLMLoss`: Standard cross-entropy loss for next-token prediction.
*   `pretrain_mode`: If `True`, trains on the entire sequence (standard language modeling). If `False` (typical for SFT), masks the loss on prompt tokens.
*   `prompt_id_lens` / `infos["response_ranges"]`: Information used to identify the prompt part of the sequence for loss masking.
*   `packing_samples`: Enables packing multiple sequences into one for potentially faster training.
